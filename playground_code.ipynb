{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy\n",
    "import config as cf\n",
    "from json import loads\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from pymongo import MongoClient,UpdateOne\n",
    "import config as cf\n",
    "import emoji\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from string import punctuation \n",
    "\n",
    "eval_obj = SentimentIntensityAnalyzer()\n",
    "api_key = cf.twitter_apikey\n",
    "api_key_secret = cf.twitter_secretapikey\n",
    "access_token = cf.twitter_accesstoken\n",
    "access_token_secret = cf.twitter_accesstokensecret\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "\n",
    "# set access to user's access key and access secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# calling the api\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "stats = api.rate_limit_status()\n",
    "\n",
    "\n",
    "\n",
    "def load_db(df,table_name):\n",
    "    db=MongoClient(cf.mdb_string)\n",
    "    mydatabase=db[cf.mdb_db]\n",
    "\n",
    "    dump_df=df \n",
    "    updates = []\n",
    "    for _, row in dump_df.iterrows():\n",
    "        if table_name==\"Trends\":\n",
    "            updates.append(UpdateOne({'Trend_Name': row.get('Trend_Name'), 'WOEID': row.get('WOEID')},\n",
    "            {\n",
    "                '$set': {\n",
    "                    'Search_Query': row.get('Search_Query'),\n",
    "                    'Total_Tweets_Count': row.get('Total_Tweets_Count'),\n",
    "                    'Pulled_At': row.get('Pulled_At')\n",
    "                    },\n",
    "                \"$currentDate\": {\"lastModified\": True}\n",
    "            }, upsert=True))\n",
    "        elif table_name==\"Tweets\":\n",
    "            updates.append(UpdateOne({'tweet_id': row.get('tweet_id'), 'WOEID': row.get('WOEID')},\n",
    "            {\n",
    "                '$set': {\n",
    "                    'tweet_created_at': row.get('tweet_created_at'),\n",
    "                    'tweet_raw_text': row.get('tweet_raw_text'),\n",
    "                    'tweet_cleaned_text': row.get('tweet_cleaned_text'),\n",
    "                    'tweet_emojis': row.get('tweet_emojis'),\n",
    "                    'tweet_hashtags': row.get('tweet_hashtags'),\n",
    "                    'tweet_compound_score':row.get('tweet_compound_score'),\n",
    "                    'tweet_lang': row.get('tweet_lang'),\n",
    "                    'tweet_source': row.get('tweet_source'),\n",
    "                    'tweet_retweet_count': row.get('tweet_retweet_count'),\n",
    "                    'tweet_like_count': row.get('tweet_like_count'),\n",
    "                    'tweet_search_query': row.get('tweet_search_query'),\n",
    "                    'tweet_scrapped_datetime_utc': row.get('tweet_scrapped_datetime_utc')\n",
    "                },\n",
    "                \"$currentDate\": {\"lastModified\": True}\n",
    "            }, upsert=True))\n",
    "    \n",
    "    mydatabase[table_name].bulk_write(updates)\n",
    "    result = mydatabase[table_name].bulk_write(updates)\n",
    "    result = result.bulk_api_result\n",
    "    # print(\"Summary Inserted: {} Updated: {} Errors: {}\".format(\n",
    "    #     max(result['nInserted'], result['nUpserted']), result['nModified'], max(result['writeErrors'], result['writeConcernErrors'])))\n",
    "    db.close()\n",
    "\n",
    "def cleanTweet(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # remove @mentions\n",
    "    text = re.sub(r'\\#', '', text)  # remove hash tag\n",
    "    text = re.sub(r'RT[\\s]+', '', text)  # remove RT\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)  # remove hyperlink\n",
    "    text = re.sub(r'\\n', ' ', text)  # remove next line character\n",
    "    text = re.sub(r'[0-9]','',text) # Remove numbers\n",
    "    text = ''.join([char for char in text if char not in punctuation ]) # remove punctuations\n",
    "    text = text.strip() #remove whitespaces\n",
    "    return text\n",
    "\n",
    "\n",
    "def read_location(filepath):\n",
    "    \n",
    "    if not path.exists(filepath):\n",
    "        raise FileExistsError(\"File Doesnot exists!! Please check filepath\")\n",
    "    locations_json=open(filepath,\"r\")\n",
    "    locations_json=loads(locations_json.read())\n",
    "    return locations_json\n",
    "\n",
    "\n",
    "def pull_trends(woeid,max_trends=50):\n",
    "    trends_list = []\n",
    "    trends=api.get_place_trends(woeid)\n",
    "    for i in range(min(len(trends[0]['trends']),max_trends)):\n",
    "        # if no tweets counts return by API skip it\n",
    "        if not trends[0]['trends'][i]['tweet_volume']:\n",
    "            continue\n",
    "        trends_list.append(\n",
    "            [trends[0]['trends'][i]['name'], trends[0]['trends'][i]['query'], trends[0]['trends'][i]['tweet_volume'], woeid, datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")])\n",
    "    return trends_list\n",
    "\n",
    "\n",
    "def pull_popular_tweets(topics_list, woeid, max_tweet_topic=20,total_max_tweets=100):\n",
    "\n",
    "    df_return = pd.DataFrame()\n",
    "\n",
    "    for topic in topics_list:\n",
    "        if len(df_return.index) == total_max_tweets:\n",
    "            break\n",
    "        tweets = tweepy.Cursor(api.search_tweets, q=topic[1],                         # search for each trending topic\n",
    "                               lang=\"en\", result_type='popular',                      # tweets in english , type is \"recent\"/\"popular\"\n",
    "                               tweet_mode='extended',count=50).items(max_tweet_topic)\n",
    "        tweets_list = list(tweets)\n",
    "        print(\"Topic: \"+topic[0]+\" Pulled_Tweets: \"+str(len(tweets_list)))\n",
    "        for tweet in tweets_list:\n",
    "            clean_text = cleanTweet(tweet.full_text)\n",
    "            temp_dict = {\n",
    "                'tweet_id': tweet.id,\n",
    "                'WOEID': woeid,\n",
    "                'tweet_created_at': tweet.created_at.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                'tweet_raw_text': tweet.full_text.replace(\"'\", \"''\"),\n",
    "                'tweet_cleaned_text': clean_text,\n",
    "                'tweet_emojis': (lambda x: ', '.join(c for c in x if c in emoji.EMOJI_DATA))(clean_text),\n",
    "                'tweet_hashtags': (lambda x: \", \".join(re.findall(r\"#(\\w+)\", x)))(tweet.full_text),\n",
    "                'tweet_compound_score': (lambda x: eval_obj.polarity_scores(x)['compound']) (clean_text),\n",
    "                'tweet_lang': tweet.lang,\n",
    "                'tweet_source': tweet.source,\n",
    "                'tweet_retweet_count': tweet.retweet_count,\n",
    "                'tweet_like_count': tweet.favorite_count,\n",
    "                'tweet_search_query': topic[0],\n",
    "                'tweet_scrapped_datetime_utc': datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            temp_df = pd.DataFrame([temp_dict])\n",
    "            df_return = pd.concat([df_return, temp_df], ignore_index=True)\n",
    "    print(\"Total Tweets Pulled: \"+str(len(df_return.index)))\n",
    "    return df_return\n",
    "\n",
    "\n",
    "def build_dataset():\n",
    "    records_trend=0\n",
    "    records_tweets=0\n",
    "    locations=read_location(\"Locations.json\")\n",
    "    for location in locations:\n",
    "        final_tweets_df = pd.DataFrame()\n",
    "        final_trends_df = pd.DataFrame()\n",
    "        print(\"\\nScraping Tweets for Location: \"+location['name'])\n",
    "        trend_list=pull_trends(location['woeid'],max_trends=20)\n",
    "        trend_df = pd.DataFrame(trend_list, columns=[\n",
    "                                'Trend_Name', 'Search_Query', 'Total_Tweets_Count', 'WOEID', 'Pulled_At'])\n",
    "        trend_df = trend_df.fillna(0)\n",
    "        \n",
    "        final_trends_df = pd.concat([final_trends_df, trend_df], ignore_index=True)\n",
    "\n",
    "        final_tweets_df=pd.concat([final_tweets_df,\n",
    "                            pull_popular_tweets(trend_list,location['woeid'],max_tweet_topic=30,total_max_tweets=10000)]\n",
    "                            ,ignore_index = True)\n",
    "        if len(final_tweets_df.index)==0:\n",
    "            continue\n",
    "        trend_df = pd.merge(trend_df\n",
    "                    , final_tweets_df.groupby(['WOEID', 'tweet_search_query']).size().reset_index(name='counts')   \n",
    "                    , how=\"left\", left_on=['WOEID', 'Trend_Name'], right_on=['WOEID', 'tweet_search_query'])\n",
    "        trend_df=trend_df.dropna()\n",
    "        records_trend+=len(final_trends_df.index)\n",
    "        records_tweets+=len(final_tweets_df.index)\n",
    "        load_db(final_tweets_df,\"Tweets\")\n",
    "        load_db(final_trends_df,\"Trends\")\n",
    "    \n",
    "    print(\"Total Records Pulled\\nTweets: {} Trends: {}\".format(records_tweets,records_trend))\n",
    "    # return final_tweets_df,final_trends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping Tweets for Location: United States\n",
      "Topic: Cowboys Pulled_Tweets: 30\n",
      "Topic: Texans Pulled_Tweets: 30\n",
      "Topic: Bears Pulled_Tweets: 30\n",
      "Topic: Brazil Pulled_Tweets: 30\n",
      "Topic: Dolphins Pulled_Tweets: 30\n",
      "Topic: Steelers Pulled_Tweets: 30\n",
      "Topic: Jets Pulled_Tweets: 30\n",
      "Topic: Lovie Smith Pulled_Tweets: 20\n",
      "Topic: Colts Pulled_Tweets: 30\n",
      "Topic: #FinsUp Pulled_Tweets: 21\n",
      "Topic: Rams Pulled_Tweets: 30\n",
      "Topic: Browns Pulled_Tweets: 30\n",
      "Topic: #HereWeGo Pulled_Tweets: 30\n",
      "Topic: Patriots Pulled_Tweets: 30\n",
      "Topic: #FlyEaglesFly Pulled_Tweets: 23\n",
      "Total Tweets Pulled: 424\n",
      "\n",
      "Scraping Tweets for Location: Canada\n",
      "Topic: Brazil Pulled_Tweets: 30\n",
      "Topic: Lula Pulled_Tweets: 30\n",
      "Topic: Jets Pulled_Tweets: 30\n",
      "Topic: #BillsMafia Pulled_Tweets: 30\n",
      "Topic: Texans Pulled_Tweets: 30\n",
      "Topic: Bears Pulled_Tweets: 30\n",
      "Topic: #FinsUp Pulled_Tweets: 21\n",
      "Topic: Cowboys Pulled_Tweets: 30\n",
      "Topic: Dolphins Pulled_Tweets: 30\n",
      "Topic: #MohammadGhobadlou Pulled_Tweets: 15\n",
      "Topic: Hines Pulled_Tweets: 30\n",
      "Topic: Lovie Smith Pulled_Tweets: 20\n",
      "Topic: Steelers Pulled_Tweets: 30\n",
      "Topic: Rams Pulled_Tweets: 30\n",
      "Topic: Brésil Pulled_Tweets: 2\n",
      "Topic: #MohammadBroghani Pulled_Tweets: 15\n",
      "Topic: Dame Pulled_Tweets: 30\n",
      "Total Tweets Pulled: 433\n",
      "\n",
      "Scraping Tweets for Location: India\n",
      "Topic: Araujo Pulled_Tweets: 19\n",
      "Topic: #SuryakumarYadav Pulled_Tweets: 21\n",
      "Topic: Savic Pulled_Tweets: 13\n",
      "Topic: Ferran Pulled_Tweets: 23\n",
      "Topic: #HBDRockingStarYash Pulled_Tweets: 14\n",
      "Topic: #VarisuBookings Pulled_Tweets: 10\n",
      "Topic: #AtletiBarca Pulled_Tweets: 13\n",
      "Topic: #ThunivuBookings Pulled_Tweets: 2\n",
      "Topic: JANTA KI JAAN PRIYANKA Pulled_Tweets: 6\n",
      "Topic: Christensen Pulled_Tweets: 13\n",
      "Topic: Dembele Pulled_Tweets: 27\n",
      "Topic: Pedri Pulled_Tweets: 26\n",
      "Topic: PROUD OF SHAH RUKH KHAN Pulled_Tweets: 13\n",
      "Topic: Chelsea Pulled_Tweets: 30\n",
      "Topic: GOAT PLAYER SHIV THAKARE Pulled_Tweets: 11\n",
      "Topic: Xavi Pulled_Tweets: 25\n",
      "Topic: Busquets Pulled_Tweets: 12\n",
      "Topic: Potter Pulled_Tweets: 30\n",
      "Total Tweets Pulled: 308\n",
      "\n",
      "Scraping Tweets for Location: Thailand\n",
      "Topic: #BORNPINKinBANGKOK Pulled_Tweets: 11\n",
      "Topic: #BetweenUsEP9 Pulled_Tweets: 0\n",
      "Topic: #WelcomeTENandRENJUNtoThailand Pulled_Tweets: 0\n",
      "Topic: #GDAinBKK Pulled_Tweets: 3\n",
      "Topic: #DestinyclinicZeeNunew Pulled_Tweets: 4\n",
      "Topic: BamBam All the Butlers EP2 Pulled_Tweets: 0\n",
      "Topic: ENGLOT DOMINATE THE WORLD Pulled_Tweets: 0\n",
      "Topic: bambam gm 08january Pulled_Tweets: 0\n",
      "Topic: LALISA THAILAND PRIDE Pulled_Tweets: 4\n",
      "Topic: gap girl love Pulled_Tweets: 0\n",
      "Topic: nene the nine tailed fox Pulled_Tweets: 0\n",
      "Topic: DAYDREAM CONCEPT PHOTO Pulled_Tweets: 2\n",
      "Topic: engfa waraha Pulled_Tweets: 0\n",
      "Topic: charlotte austin Pulled_Tweets: 0\n",
      "Topic: SOLVE THE PUZZLE Pulled_Tweets: 2\n",
      "Topic: Chelsea Pulled_Tweets: 30\n",
      "Total Tweets Pulled: 56\n",
      "\n",
      "Scraping Tweets for Location: Australia\n",
      "Topic: Brazil Pulled_Tweets: 30\n",
      "Topic: Dutton Pulled_Tweets: 29\n",
      "Topic: Agar Pulled_Tweets: 10\n",
      "Topic: Jair Bolsonaro Pulled_Tweets: 30\n",
      "Topic: Australia Day Pulled_Tweets: 2\n",
      "Topic: Lula Pulled_Tweets: 30\n",
      "Topic: #FinsUp Pulled_Tweets: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: Florida Pulled_Tweets: 30\n",
      "Topic: Colts Pulled_Tweets: 30\n",
      "Topic: Bears Pulled_Tweets: 30\n",
      "Topic: Texans Pulled_Tweets: 30\n",
      "Topic: THEY CAN'T BEAT US Pulled_Tweets: 4\n",
      "Total Tweets Pulled: 276\n",
      "\n",
      "Scraping Tweets for Location: Kenya\n",
      "Topic: Chelsea Pulled_Tweets: 30\n",
      "Topic: #MCICHE Pulled_Tweets: 14\n",
      "Topic: Potter Pulled_Tweets: 30\n",
      "Topic: Tuchel Pulled_Tweets: 29\n",
      "Topic: LGBTQ Pulled_Tweets: 30\n",
      "Topic: #FACup Pulled_Tweets: 29\n",
      "Topic: Mahrez Pulled_Tweets: 30\n",
      "Topic: Gallagher Pulled_Tweets: 30\n",
      "Topic: Blessed Sunday Pulled_Tweets: 16\n",
      "Topic: Todd Boehly Pulled_Tweets: 30\n",
      "Topic: Barca Pulled_Tweets: 30\n",
      "Total Tweets Pulled: 298\n",
      "\n",
      "Scraping Tweets for Location: United Kingdom\n",
      "Topic: #HarryTheInterview Pulled_Tweets: 14\n",
      "Topic: Brazil Pulled_Tweets: 30\n",
      "Topic: Oprah Pulled_Tweets: 30\n",
      "Topic: Stevenage Pulled_Tweets: 29\n",
      "Topic: Chelsea Pulled_Tweets: 30\n",
      "Topic: Camilla Pulled_Tweets: 30\n",
      "Topic: Savic Pulled_Tweets: 14\n",
      "Topic: Lula Pulled_Tweets: 30\n",
      "Topic: Texans Pulled_Tweets: 30\n",
      "Topic: Roma Pulled_Tweets: 30\n",
      "Topic: Tuchel Pulled_Tweets: 29\n",
      "Total Tweets Pulled: 296\n",
      "\n",
      "Scraping Tweets for Location: Portugal\n",
      "Topic: Sporting Pulled_Tweets: 30\n",
      "Topic: Brasília Pulled_Tweets: 30\n",
      "Topic: Lula Pulled_Tweets: 30\n",
      "Topic: Savic Pulled_Tweets: 14\n",
      "Topic: Dino Pulled_Tweets: 23\n",
      "Topic: Porto Pulled_Tweets: 15\n",
      "Total Tweets Pulled: 142\n",
      "\n",
      "Scraping Tweets for Location: Singapore\n",
      "Topic: #AOMGWORLDTOUR2023inSG Pulled_Tweets: 3\n",
      "Topic: yugyeom Pulled_Tweets: 13\n",
      "Topic: #AlchemyOfSouls2Ep10 Pulled_Tweets: 0\n",
      "Topic: #TXT_Daydream Pulled_Tweets: 4\n",
      "Topic: Potter Pulled_Tweets: 30\n",
      "Topic: DAYDREAM CONCEPT PHOTO Pulled_Tweets: 2\n",
      "Topic: Chelsea Pulled_Tweets: 30\n",
      "Topic: THEY CAN'T BEAT US Pulled_Tweets: 4\n",
      "Topic: Gray Pulled_Tweets: 30\n",
      "Topic: Don Quixote Pulled_Tweets: 5\n",
      "Topic: Tuchel Pulled_Tweets: 29\n",
      "Topic: wjsn Pulled_Tweets: 7\n",
      "Topic: jeon wonwoo Pulled_Tweets: 0\n",
      "Topic: hobi Pulled_Tweets: 12\n",
      "Topic: seokjin Pulled_Tweets: 8\n",
      "Total Tweets Pulled: 177\n",
      "\n",
      "Scraping Tweets for Location: Malaysia\n",
      "Topic: #AlchemyOfSouls2Ep10 Pulled_Tweets: 0\n",
      "Topic: #TXT_Daydream Pulled_Tweets: 4\n",
      "Topic: #TEMPTATION Pulled_Tweets: 10\n",
      "Topic: DAYDREAM CONCEPT PHOTO Pulled_Tweets: 2\n",
      "Topic: #TOMORROW_X_TOGETHER Pulled_Tweets: 10\n",
      "Topic: Chelsea Pulled_Tweets: 30\n",
      "Topic: Potter Pulled_Tweets: 30\n",
      "Topic: Tuchel Pulled_Tweets: 29\n",
      "Topic: SOLVE THE PUZZLE Pulled_Tweets: 2\n",
      "Topic: Jang Uk Pulled_Tweets: 1\n",
      "Topic: yeonjun Pulled_Tweets: 8\n",
      "Topic: Hong Pulled_Tweets: 30\n",
      "Topic: HAPPY ENDING Pulled_Tweets: 16\n",
      "Topic: Thailand Pulled_Tweets: 30\n",
      "Topic: Park Jin Pulled_Tweets: 0\n",
      "Topic: THEY CAN'T BEAT US Pulled_Tweets: 4\n",
      "Topic: Mahrez Pulled_Tweets: 30\n",
      "Topic: wjsn Pulled_Tweets: 7\n",
      "Topic: jay park Pulled_Tweets: 5\n",
      "Total Tweets Pulled: 248\n",
      "\n",
      "Scraping Tweets for Location: South Africa\n",
      "Topic: Chelsea Pulled_Tweets: 30\n",
      "Topic: #MCICHE Pulled_Tweets: 14\n",
      "Topic: Potter Pulled_Tweets: 30\n",
      "Topic: Pedri Pulled_Tweets: 26\n",
      "Topic: Savic Pulled_Tweets: 14\n",
      "Topic: Tuchel Pulled_Tweets: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: Dembele Pulled_Tweets: 27\n",
      "Topic: Mahrez Pulled_Tweets: 30\n",
      "Topic: Xavi Pulled_Tweets: 24\n",
      "Topic: Barca Pulled_Tweets: 30\n",
      "Topic: Ferran Pulled_Tweets: 23\n",
      "Topic: Atletico Pulled_Tweets: 28\n",
      "Topic: Alvarez Pulled_Tweets: 27\n",
      "Total Tweets Pulled: 332\n",
      "\n",
      "Scraping Tweets for Location: Peru\n",
      "Topic: Lula Pulled_Tweets: 30\n",
      "Topic: Savic Pulled_Tweets: 15\n",
      "Topic: Junior Pulled_Tweets: 30\n",
      "Topic: Juliaca Pulled_Tweets: 0\n",
      "Topic: Andrade Pulled_Tweets: 9\n",
      "Topic: Ferran Pulled_Tweets: 23\n",
      "Topic: #TEMPTATION Pulled_Tweets: 10\n",
      "Topic: #TXT_Daydream Pulled_Tweets: 4\n",
      "Topic: Brasilia Pulled_Tweets: 30\n",
      "Topic: Pedri Pulled_Tweets: 26\n",
      "Total Tweets Pulled: 177\n",
      "\n",
      "Scraping Tweets for Location: New Zealand\n",
      "Topic: Brazil Pulled_Tweets: 30\n",
      "Topic: Democracy Pulled_Tweets: 30\n",
      "Topic: FA Cup Pulled_Tweets: 30\n",
      "Topic: Florida Pulled_Tweets: 30\n",
      "Topic: McCarthy Pulled_Tweets: 30\n",
      "Topic: Lakers Pulled_Tweets: 30\n",
      "Topic: Andrew Tate Pulled_Tweets: 30\n",
      "Topic: Elliot Pulled_Tweets: 29\n",
      "Topic: Potter Pulled_Tweets: 30\n",
      "Topic: Michael Pulled_Tweets: 30\n",
      "Topic: Coffee Pulled_Tweets: 30\n",
      "Topic: NFTs Pulled_Tweets: 30\n",
      "Topic: Epstein Pulled_Tweets: 27\n",
      "Total Tweets Pulled: 386\n",
      "\n",
      "Scraping Tweets for Location: Sweden\n",
      "Topic: Lula Pulled_Tweets: 30\n",
      "Topic: Roma Pulled_Tweets: 30\n",
      "Topic: NATO Pulled_Tweets: 30\n",
      "Topic: Brazil Pulled_Tweets: 30\n",
      "Topic: Potter Pulled_Tweets: 30\n",
      "Topic: Brasilia Pulled_Tweets: 30\n",
      "Topic: Milan Pulled_Tweets: 29\n",
      "Topic: Atletico Pulled_Tweets: 28\n",
      "Topic: Ante Pulled_Tweets: 14\n",
      "Topic: Wolves Pulled_Tweets: 26\n",
      "Topic: Tuchel Pulled_Tweets: 29\n",
      "Total Tweets Pulled: 306\n",
      "\n",
      "Scraping Tweets for Location: Brazil\n",
      "Topic: Brasília Pulled_Tweets: 30\n",
      "Topic: SEM ANISTIA Pulled_Tweets: 0\n",
      "Topic: Ibaneis Pulled_Tweets: 1\n",
      "Topic: Bolsonarismo Pulled_Tweets: 3\n",
      "Topic: Planalto Pulled_Tweets: 21\n",
      "Topic: Xandão Pulled_Tweets: 0\n",
      "Topic: Distrito Federal Pulled_Tweets: 1\n",
      "Topic: Anderson Torres Pulled_Tweets: 4\n",
      "Topic: Cala Pulled_Tweets: 1\n",
      "Topic: Congresso Pulled_Tweets: 5\n",
      "Topic: Democracia Pulled_Tweets: 2\n",
      "Topic: Globo Pulled_Tweets: 12\n",
      "Topic: Alexandre de Moraes Pulled_Tweets: 7\n",
      "Topic: Galoucura Pulled_Tweets: 0\n",
      "Topic: Monark Pulled_Tweets: 1\n",
      "Topic: #Brazil Pulled_Tweets: 30\n",
      "Topic: vingadores Pulled_Tweets: 0\n",
      "Topic: #GolpeDeEstado Pulled_Tweets: 0\n",
      "Topic: Capitólio Pulled_Tweets: 0\n",
      "Total Tweets Pulled: 118\n",
      "\n",
      "Scraping Tweets for Location: Italy\n",
      "Topic: #MilanRoma Pulled_Tweets: 16\n",
      "Topic: #Brasile Pulled_Tweets: 1\n",
      "Topic: Lula Pulled_Tweets: 30\n",
      "Topic: #brasilia Pulled_Tweets: 15\n",
      "Topic: Abraham Pulled_Tweets: 30\n",
      "Topic: Tammy Pulled_Tweets: 27\n",
      "Topic: Matic Pulled_Tweets: 30\n",
      "Topic: Napoli Pulled_Tweets: 30\n",
      "Total Tweets Pulled: 179\n",
      "\n",
      "Scraping Tweets for Location: Japan\n",
      "Topic: 成人の日 Pulled_Tweets: 0\n",
      "Total Tweets Pulled: 0\n",
      "\n",
      "Scraping Tweets for Location: France\n",
      "Topic: Zidane Pulled_Tweets: 28\n",
      "Topic: Le Graet Pulled_Tweets: 12\n",
      "Topic: Zizou Pulled_Tweets: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: Kylian Pulled_Tweets: 27\n",
      "Topic: #Bresil Pulled_Tweets: 0\n",
      "Topic: Araujo Pulled_Tweets: 19\n",
      "Topic: Lula Pulled_Tweets: 30\n",
      "Topic: Deschamps Pulled_Tweets: 17\n",
      "Topic: Ferran Pulled_Tweets: 23\n",
      "Topic: Benzema Pulled_Tweets: 28\n",
      "Topic: Xavi Pulled_Tweets: 24\n",
      "Topic: Savic Pulled_Tweets: 15\n",
      "Topic: #brasilia Pulled_Tweets: 15\n",
      "Topic: #AtletiBarca Pulled_Tweets: 13\n",
      "Topic: Molina Pulled_Tweets: 10\n",
      "Total Tweets Pulled: 262\n",
      "\n",
      "Scraping Tweets for Location: Germany\n",
      "Topic: #MohammadGhobadlou Pulled_Tweets: 19\n",
      "Topic: #MohammadBroghani Pulled_Tweets: 16\n",
      "Topic: Lula Pulled_Tweets: 30\n",
      "Topic: Dolphins Pulled_Tweets: 30\n",
      "Topic: Sauna Pulled_Tweets: 12\n",
      "Topic: Texans Pulled_Tweets: 30\n",
      "Topic: THEY CAN'T BEAT US Pulled_Tweets: 4\n",
      "Topic: Bears Pulled_Tweets: 30\n",
      "Topic: Playoffs Pulled_Tweets: 30\n",
      "Total Tweets Pulled: 201\n",
      "\n",
      "Scraping Tweets for Location: Turkey\n",
      "Topic: #FBvGS Pulled_Tweets: 8\n",
      "Topic: Galatasaray Pulled_Tweets: 13\n",
      "Topic: Ali Koç Pulled_Tweets: 0\n",
      "Topic: Fenerbahçe Pulled_Tweets: 21\n",
      "Topic: #Eytseçimibekliyor Pulled_Tweets: 0\n",
      "Topic: Altay Pulled_Tweets: 1\n",
      "Topic: Albertinein Pulled_Tweets: 0\n",
      "Topic: Jesus Pulled_Tweets: 30\n",
      "Topic: Admin Pulled_Tweets: 30\n",
      "Topic: Okan Pulled_Tweets: 1\n",
      "Topic: Kadıköy Pulled_Tweets: 0\n",
      "Topic: Arda Pulled_Tweets: 6\n",
      "Total Tweets Pulled: 110\n",
      "\n",
      "Scraping Tweets for Location: Ukraine\n",
      "Topic: Kramatorsk Pulled_Tweets: 30\n",
      "Topic: #CapyHolidays Pulled_Tweets: 3\n",
      "Topic: Iran Pulled_Tweets: 30\n",
      "Topic: THEY CAN'T BEAT US Pulled_Tweets: 4\n",
      "Topic: Merry Christmas Pulled_Tweets: 30\n",
      "Topic: Leopard Pulled_Tweets: 30\n",
      "Topic: Soledar Pulled_Tweets: 24\n",
      "Total Tweets Pulled: 151\n",
      "Total Records Pulled\n",
      "Tweets: 4880 Trends: 257\n"
     ]
    }
   ],
   "source": [
    "build_dataset()\n",
    "# temp_tweet_df,trend_temp_df=build_dataset()\n",
    "# load_db(temp_tweet_df,\"Tweets\")\n",
    "# load_db(trend_temp_df,\"Trends\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping Tweets for Location: Germany\n",
      "Topic: #3K23 Pulled_Tweets: 0\n",
      "Topic: #DieZukunftGlaubtAnUns Pulled_Tweets: 0\n",
      "Topic: Feiertag Pulled_Tweets: 0\n",
      "Topic: #Marder Pulled_Tweets: 11\n",
      "Topic: Vornamen Pulled_Tweets: 0\n",
      "Topic: Panzer Pulled_Tweets: 3\n",
      "Topic: Kassiererin Pulled_Tweets: 0\n",
      "Topic: #FreeNella Pulled_Tweets: 0\n",
      "Topic: #MohammadBroghani Pulled_Tweets: 4\n",
      "Topic: Kriegspartei Pulled_Tweets: 0\n",
      "Topic: Kriegstreiber Pulled_Tweets: 0\n",
      "Topic: Waffenruhe Pulled_Tweets: 0\n",
      "Topic: Bert Pulled_Tweets: 16\n",
      "Topic: AND MY PLEASURE Pulled_Tweets: 3\n",
      "Topic: start ins wochenende Pulled_Tweets: 0\n",
      "Topic: Russland Pulled_Tweets: 3\n",
      "Topic: Erfrischungsstäbchen Pulled_Tweets: 0\n",
      "Topic: Könige Pulled_Tweets: 0\n",
      "Topic: Bewerbungen Pulled_Tweets: 0\n",
      "Topic: Paula Pulled_Tweets: 14\n",
      "Total Tweets Pulled: 54\n",
      "           Trend_Name           Search_Query  Total_Tweets_Count     WOEID  \\\n",
      "3             #Marder              %23Marder                 0.0  23424829   \n",
      "5              Panzer                 Panzer             15350.0  23424829   \n",
      "8   #MohammadBroghani    %23MohammadBroghani                 0.0  23424829   \n",
      "12               Bert                   Bert             18665.0  23424829   \n",
      "13    AND MY PLEASURE  %22AND+MY+PLEASURE%22            110030.0  23424829   \n",
      "15           Russland               Russland             17698.0  23424829   \n",
      "19              Paula                  Paula             37841.0  23424829   \n",
      "\n",
      "              Pulled_At tweet_search_query  counts  \n",
      "3   2023-01-07 02:43:40            #Marder    11.0  \n",
      "5   2023-01-07 02:43:40             Panzer     3.0  \n",
      "8   2023-01-07 02:43:40  #MohammadBroghani     4.0  \n",
      "12  2023-01-07 02:43:40               Bert    16.0  \n",
      "13  2023-01-07 02:43:40    AND MY PLEASURE     3.0  \n",
      "15  2023-01-07 02:43:40           Russland     3.0  \n",
      "19  2023-01-07 02:43:40              Paula    14.0  \n",
      "\n",
      "Scraping Tweets for Location: Turkey\n",
      "Topic: #Okageinu Pulled_Tweets: 0\n",
      "Topic: #deprem Pulled_Tweets: 1\n",
      "Topic: #EytOcaktaEmekliOlsun Pulled_Tweets: 0\n",
      "Topic: Belkide Gunesin Pulled_Tweets: 0\n",
      "Topic: #BuZamMemuraHakaret Pulled_Tweets: 0\n",
      "Topic: #Gündem100BinÖğretmen Pulled_Tweets: 0\n",
      "Topic: hayırlı cumalar Pulled_Tweets: 0\n",
      "Topic: Halil Umut Meler Pulled_Tweets: 0\n",
      "Topic: Midilli Pulled_Tweets: 0\n",
      "Topic: Mehmet Karahanlı Pulled_Tweets: 0\n",
      "Topic: zekatla bereketlensin Pulled_Tweets: 0\n",
      "Topic: Dilek Sabancı Pulled_Tweets: 0\n",
      "Topic: esnafsuçlu değilborçlu Pulled_Tweets: 0\n",
      "Topic: YusufAnnesiyle İyileşsin Pulled_Tweets: 0\n",
      "Topic: Mansur Yavaş Pulled_Tweets: 0\n",
      "Topic: Cumhurbaşkanı Pulled_Tweets: 0\n",
      "Topic: Ferit Pulled_Tweets: 1\n",
      "Topic: Davutoğlu Pulled_Tweets: 2\n",
      "Topic: Başınız Pulled_Tweets: 0\n",
      "Topic: cinsel halt partisi Pulled_Tweets: 0\n",
      "Total Tweets Pulled: 4\n",
      "   Trend_Name    Search_Query  Total_Tweets_Count     WOEID  \\\n",
      "1     #deprem       %23deprem                 0.0  23424969   \n",
      "16      Ferit           Ferit             26566.0  23424969   \n",
      "17  Davutoğlu  Davuto%C4%9Flu             81257.0  23424969   \n",
      "\n",
      "              Pulled_At tweet_search_query  counts  \n",
      "1   2023-01-07 02:43:45            #deprem     1.0  \n",
      "16  2023-01-07 02:43:45              Ferit     1.0  \n",
      "17  2023-01-07 02:43:45          Davutoğlu     2.0  \n",
      "\n",
      "Scraping Tweets for Location: Ukraine\n",
      "Topic: Orthodox Christmas Pulled_Tweets: 30\n",
      "Topic: Рождество Pulled_Tweets: 0\n",
      "Topic: Easter Pulled_Tweets: 29\n",
      "Topic: #Ukraina Pulled_Tweets: 0\n",
      "Topic: #boxing Pulled_Tweets: 15\n",
      "Topic: #Putin Pulled_Tweets: 25\n",
      "Topic: #Belarus Pulled_Tweets: 27\n",
      "Topic: Bradley Pulled_Tweets: 30\n",
      "Topic: Christian Pulled_Tweets: 30\n",
      "Topic: Patriot Pulled_Tweets: 30\n",
      "Topic: Lviv Pulled_Tweets: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 580\n"
     ]
    }
   ],
   "source": [
    "def build_dataset():\n",
    "    records_trend=0\n",
    "    records_tweets=0\n",
    "    locations=read_location(\"Locations.json\")\n",
    "    final_tweets_df = pd.DataFrame()\n",
    "    final_trends_df = pd.DataFrame()\n",
    "    for location in locations:\n",
    "        \n",
    "        print(\"\\nScraping Tweets for Location: \"+location['name'])\n",
    "        trend_list=pull_trends(location['woeid'],max_trends=20)\n",
    "        trend_df = pd.DataFrame(trend_list, columns=[\n",
    "                                'Trend_Name', 'Search_Query', 'Total_Tweets_Count', 'WOEID', 'Pulled_At'])\n",
    "        trend_df = trend_df.fillna(0)\n",
    "        \n",
    "        final_trends_df = pd.concat([final_trends_df, trend_df], ignore_index=True)\n",
    "\n",
    "        final_tweets_df=pd.concat([final_tweets_df,\n",
    "                            pull_popular_tweets(trend_list,location['woeid'],max_tweet_topic=30,total_max_tweets=10000)]\n",
    "                            ,ignore_index = True)\n",
    "\n",
    "        trend_df = pd.merge(trend_df\n",
    "                   , final_tweets_df.groupby(['WOEID', 'tweet_search_query']).size().reset_index(name='counts')   \n",
    "                   , how=\"left\", left_on=['WOEID', 'Trend_Name'], right_on=['WOEID', 'tweet_search_query'])\n",
    "        trend_df=trend_df.dropna()\n",
    "        print(trend_df)\n",
    "        records_trend+=len(final_trends_df.index)\n",
    "        records_tweets+=len(final_tweets_df.index)\n",
    "        # load_db(final_tweets_df,\"Tweets3\")\n",
    "        # load_db(final_trends_df,\"Trends3\")\n",
    "    \n",
    "    print(\"Total Records Pulled\\nTweets: {} Trends: {}\".format(records_tweets,records_trend))\n",
    "    return final_tweets_df,final_trends_df\n",
    "temp_tweet_df,trend_temp_df=build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Trend_Name', 'WOEID', 'Pulled_At', 'Search_Query',\n",
       "       'Total_Tweets_Count', 'lastModified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "client = MongoClient(config.mdb_string)\n",
    "mydatabase = client[config.mdb_db]\n",
    "collection = mydatabase[\"Tweets\"]\n",
    "all_record = collection.find()\n",
    "\n",
    "df_tweets = pd.DataFrame(list(collection.find()))\n",
    "df_tweets = df_tweets.drop(columns=['_id'])\n",
    "\n",
    "collection = mydatabase[\"Trends\"]\n",
    "all_record = collection.find()\n",
    "\n",
    "df_trends = pd.DataFrame(list(collection.find()))\n",
    "df_trends = df_trends.drop(columns=['_id'])\n",
    "df_trends.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trend_Name</th>\n",
       "      <th>WOEID</th>\n",
       "      <th>Pulled_At</th>\n",
       "      <th>Search_Query</th>\n",
       "      <th>Total_Tweets_Count</th>\n",
       "      <th>lastModified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pistons</td>\n",
       "      <td>23424977</td>\n",
       "      <td>2023-01-05 06:37:20</td>\n",
       "      <td>Pistons</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 03:02:02.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hannity</td>\n",
       "      <td>23424977</td>\n",
       "      <td>2023-01-05 06:37:20</td>\n",
       "      <td>Hannity</td>\n",
       "      <td>52087.0</td>\n",
       "      <td>2023-01-07 03:02:02.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#LakeShow</td>\n",
       "      <td>23424977</td>\n",
       "      <td>2023-01-05 06:37:20</td>\n",
       "      <td>%23LakeShow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 03:02:02.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Klay</td>\n",
       "      <td>23424977</td>\n",
       "      <td>2023-01-05 06:37:20</td>\n",
       "      <td>Klay</td>\n",
       "      <td>28488.0</td>\n",
       "      <td>2023-01-07 03:02:02.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Poole</td>\n",
       "      <td>23424977</td>\n",
       "      <td>2023-01-05 06:37:20</td>\n",
       "      <td>Poole</td>\n",
       "      <td>14481.0</td>\n",
       "      <td>2023-01-07 03:02:02.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>Wagner</td>\n",
       "      <td>23424976</td>\n",
       "      <td>2023-01-07 17:57:36</td>\n",
       "      <td>Wagner</td>\n",
       "      <td>34740.0</td>\n",
       "      <td>2023-01-07 17:57:52.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>Earth</td>\n",
       "      <td>23424976</td>\n",
       "      <td>2023-01-07 17:57:36</td>\n",
       "      <td>Earth</td>\n",
       "      <td>185230.0</td>\n",
       "      <td>2023-01-07 17:57:52.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>#Bakhmut</td>\n",
       "      <td>23424976</td>\n",
       "      <td>2023-01-07 17:57:36</td>\n",
       "      <td>%23Bakhmut</td>\n",
       "      <td>11836.0</td>\n",
       "      <td>2023-01-07 17:57:52.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>Jesus</td>\n",
       "      <td>23424976</td>\n",
       "      <td>2023-01-07 17:57:36</td>\n",
       "      <td>Jesus</td>\n",
       "      <td>245974.0</td>\n",
       "      <td>2023-01-07 17:57:52.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>Foundation</td>\n",
       "      <td>23424976</td>\n",
       "      <td>2023-01-07 17:57:36</td>\n",
       "      <td>Foundation</td>\n",
       "      <td>87951.0</td>\n",
       "      <td>2023-01-07 17:57:52.062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1099 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Trend_Name     WOEID            Pulled_At Search_Query  \\\n",
       "0        Pistons  23424977  2023-01-05 06:37:20      Pistons   \n",
       "1        Hannity  23424977  2023-01-05 06:37:20      Hannity   \n",
       "2      #LakeShow  23424977  2023-01-05 06:37:20  %23LakeShow   \n",
       "3           Klay  23424977  2023-01-05 06:37:20         Klay   \n",
       "4          Poole  23424977  2023-01-05 06:37:20        Poole   \n",
       "...          ...       ...                  ...          ...   \n",
       "1094      Wagner  23424976  2023-01-07 17:57:36       Wagner   \n",
       "1095       Earth  23424976  2023-01-07 17:57:36        Earth   \n",
       "1096    #Bakhmut  23424976  2023-01-07 17:57:36   %23Bakhmut   \n",
       "1097       Jesus  23424976  2023-01-07 17:57:36        Jesus   \n",
       "1098  Foundation  23424976  2023-01-07 17:57:36   Foundation   \n",
       "\n",
       "      Total_Tweets_Count            lastModified  \n",
       "0                    0.0 2023-01-07 03:02:02.545  \n",
       "1                52087.0 2023-01-07 03:02:02.545  \n",
       "2                    0.0 2023-01-07 03:02:02.545  \n",
       "3                28488.0 2023-01-07 03:02:02.545  \n",
       "4                14481.0 2023-01-07 03:02:02.545  \n",
       "...                  ...                     ...  \n",
       "1094             34740.0 2023-01-07 17:57:52.060  \n",
       "1095            185230.0 2023-01-07 17:57:52.061  \n",
       "1096             11836.0 2023-01-07 17:57:52.061  \n",
       "1097            245974.0 2023-01-07 17:57:52.062  \n",
       "1098             87951.0 2023-01-07 17:57:52.062  \n",
       "\n",
       "[1099 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.merge(df_trends\n",
    "                   , df_tweets.groupby(['WOEID', 'tweet_search_query']).size().reset_index(name='counts')   \n",
    "                   , how=\"left\", left_on=['WOEID', 'Trend_Name'], right_on=['WOEID', 'tweet_search_query'])\n",
    "df_trends=temp_df.dropna()\n",
    "df_trends=df_trends[['Trend_Name', 'WOEID', 'Pulled_At', 'Search_Query',\n",
    "           'Total_Tweets_Count', 'lastModified']]\n",
    "df_trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trend_Name</th>\n",
       "      <th>WOEID</th>\n",
       "      <th>Pulled_At</th>\n",
       "      <th>Search_Query</th>\n",
       "      <th>Total_Tweets_Count</th>\n",
       "      <th>lastModified</th>\n",
       "      <th>tweet_search_query</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#dreamspace</td>\n",
       "      <td>23424977</td>\n",
       "      <td>2023-01-05 06:37:20</td>\n",
       "      <td>%23dreamspace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-05 06:37:35.670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>#dreamspace</td>\n",
       "      <td>23424775</td>\n",
       "      <td>2023-01-05 06:37:35</td>\n",
       "      <td>%23dreamspace</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-05 06:37:48.921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>#CanadaVsUSA</td>\n",
       "      <td>23424775</td>\n",
       "      <td>2023-01-05 06:37:35</td>\n",
       "      <td>%23CanadaVsUSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-05 06:37:48.921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Airtel5G Plus In Bhubaneswar</td>\n",
       "      <td>23424848</td>\n",
       "      <td>2023-01-05 06:37:49</td>\n",
       "      <td>%22Airtel5G+Plus+In+Bhubaneswar%22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-05 06:37:58.504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>PoliticalWill Lacking InSSRCs</td>\n",
       "      <td>23424848</td>\n",
       "      <td>2023-01-05 06:37:49</td>\n",
       "      <td>%22PoliticalWill+Lacking+InSSRCs%22</td>\n",
       "      <td>21586.0</td>\n",
       "      <td>2023-01-05 06:37:58.505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>Prens Harry</td>\n",
       "      <td>23424969</td>\n",
       "      <td>2023-01-07 02:09:24</td>\n",
       "      <td>%22Prens+Harry%22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:09:28.772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>#Ukraina</td>\n",
       "      <td>23424976</td>\n",
       "      <td>2023-01-07 02:09:28</td>\n",
       "      <td>%23Ukraina</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:09:43.267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>може</td>\n",
       "      <td>23424976</td>\n",
       "      <td>2023-01-07 02:09:28</td>\n",
       "      <td>%D0%BC%D0%BE%D0%B6%D0%B5</td>\n",
       "      <td>10595.0</td>\n",
       "      <td>2023-01-07 02:09:43.270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>#cellulite</td>\n",
       "      <td>23424976</td>\n",
       "      <td>2023-01-07 02:09:28</td>\n",
       "      <td>%23cellulite</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:09:43.271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>германия</td>\n",
       "      <td>23424976</td>\n",
       "      <td>2023-01-07 02:09:28</td>\n",
       "      <td>%D0%B3%D0%B5%D1%80%D0%BC%D0%B0%D0%BD%D0%B8%D1%8F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:09:43.272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Trend_Name     WOEID            Pulled_At  \\\n",
       "13                      #dreamspace  23424977  2023-01-05 06:37:20   \n",
       "32                      #dreamspace  23424775  2023-01-05 06:37:35   \n",
       "35                     #CanadaVsUSA  23424775  2023-01-05 06:37:35   \n",
       "43     Airtel5G Plus In Bhubaneswar  23424848  2023-01-05 06:37:49   \n",
       "47    PoliticalWill Lacking InSSRCs  23424848  2023-01-05 06:37:49   \n",
       "...                             ...       ...                  ...   \n",
       "1358                    Prens Harry  23424969  2023-01-07 02:09:24   \n",
       "1359                       #Ukraina  23424976  2023-01-07 02:09:28   \n",
       "1364                           може  23424976  2023-01-07 02:09:28   \n",
       "1365                     #cellulite  23424976  2023-01-07 02:09:28   \n",
       "1366                       германия  23424976  2023-01-07 02:09:28   \n",
       "\n",
       "                                          Search_Query  Total_Tweets_Count  \\\n",
       "13                                       %23dreamspace                 0.0   \n",
       "32                                       %23dreamspace                 0.0   \n",
       "35                                      %23CanadaVsUSA                 0.0   \n",
       "43                  %22Airtel5G+Plus+In+Bhubaneswar%22                 0.0   \n",
       "47                 %22PoliticalWill+Lacking+InSSRCs%22             21586.0   \n",
       "...                                                ...                 ...   \n",
       "1358                                 %22Prens+Harry%22                 0.0   \n",
       "1359                                        %23Ukraina                 0.0   \n",
       "1364                          %D0%BC%D0%BE%D0%B6%D0%B5             10595.0   \n",
       "1365                                      %23cellulite                 0.0   \n",
       "1366  %D0%B3%D0%B5%D1%80%D0%BC%D0%B0%D0%BD%D0%B8%D1%8F                 0.0   \n",
       "\n",
       "                lastModified tweet_search_query  counts  \n",
       "13   2023-01-05 06:37:35.670                NaN     NaN  \n",
       "32   2023-01-05 06:37:48.921                NaN     NaN  \n",
       "35   2023-01-05 06:37:48.921                NaN     NaN  \n",
       "43   2023-01-05 06:37:58.504                NaN     NaN  \n",
       "47   2023-01-05 06:37:58.505                NaN     NaN  \n",
       "...                      ...                ...     ...  \n",
       "1358 2023-01-07 02:09:28.772                NaN     NaN  \n",
       "1359 2023-01-07 02:09:43.267                NaN     NaN  \n",
       "1364 2023-01-07 02:09:43.270                NaN     NaN  \n",
       "1365 2023-01-07 02:09:43.271                NaN     NaN  \n",
       "1366 2023-01-07 02:09:43.272                NaN     NaN  \n",
       "\n",
       "[429 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.merge(df_trends\n",
    "                   , df_tweets.groupby(['WOEID', 'tweet_search_query']).size().reset_index(name='counts')   \n",
    "                   , how=\"left\", left_on=['WOEID', 'Trend_Name'], right_on=['WOEID', 'tweet_search_query'])\n",
    "temp_df[temp_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trend_Name</th>\n",
       "      <th>WOEID</th>\n",
       "      <th>Pulled_At</th>\n",
       "      <th>Search_Query</th>\n",
       "      <th>Total_Tweets_Count</th>\n",
       "      <th>lastModified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#LBCPSG</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%23LBCPSG</td>\n",
       "      <td>14907.0</td>\n",
       "      <td>2023-01-07 02:14:14.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Châteauroux</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Ch%C3%A2teauroux</td>\n",
       "      <td>20900.0</td>\n",
       "      <td>2023-01-07 02:14:14.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gharbi</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Gharbi</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sarabia</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Sarabia</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#CannesComedyShow</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%23CannesComedyShow</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#VendrediLecture</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%23VendrediLecture</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Epiphanie</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%23Epiphanie</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#CoupeDeFrance</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%23CoupeDeFrance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernat</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Bernat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Emery</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Emery</td>\n",
       "      <td>11731.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nolan roux</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%22nolan+roux%22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Housni</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Housni</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mukiele</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Mukiele</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AND MY PLEASURE</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%22AND+MY+PLEASURE%22</td>\n",
       "      <td>107711.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ginny</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Ginny</td>\n",
       "      <td>50424.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Aya Nakamura</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%22Aya+Nakamura%22</td>\n",
       "      <td>14664.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Chris Evans</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%22Chris+Evans%22</td>\n",
       "      <td>17321.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Raffarin</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Raffarin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>El Chadaille</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>%22El+Chadaille%22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lavine</td>\n",
       "      <td>23424819</td>\n",
       "      <td>2023-01-07 02:14:06</td>\n",
       "      <td>Lavine</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-01-07 02:14:14.184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Trend_Name     WOEID            Pulled_At           Search_Query  \\\n",
       "0             #LBCPSG  23424819  2023-01-07 02:14:06              %23LBCPSG   \n",
       "1         Châteauroux  23424819  2023-01-07 02:14:06       Ch%C3%A2teauroux   \n",
       "2              Gharbi  23424819  2023-01-07 02:14:06                 Gharbi   \n",
       "3             Sarabia  23424819  2023-01-07 02:14:06                Sarabia   \n",
       "4   #CannesComedyShow  23424819  2023-01-07 02:14:06    %23CannesComedyShow   \n",
       "5    #VendrediLecture  23424819  2023-01-07 02:14:06     %23VendrediLecture   \n",
       "6          #Epiphanie  23424819  2023-01-07 02:14:06           %23Epiphanie   \n",
       "7      #CoupeDeFrance  23424819  2023-01-07 02:14:06       %23CoupeDeFrance   \n",
       "8              Bernat  23424819  2023-01-07 02:14:06                 Bernat   \n",
       "9               Emery  23424819  2023-01-07 02:14:06                  Emery   \n",
       "10         nolan roux  23424819  2023-01-07 02:14:06       %22nolan+roux%22   \n",
       "11             Housni  23424819  2023-01-07 02:14:06                 Housni   \n",
       "12            Mukiele  23424819  2023-01-07 02:14:06                Mukiele   \n",
       "13    AND MY PLEASURE  23424819  2023-01-07 02:14:06  %22AND+MY+PLEASURE%22   \n",
       "14              Ginny  23424819  2023-01-07 02:14:06                  Ginny   \n",
       "15       Aya Nakamura  23424819  2023-01-07 02:14:06     %22Aya+Nakamura%22   \n",
       "16        Chris Evans  23424819  2023-01-07 02:14:06      %22Chris+Evans%22   \n",
       "17           Raffarin  23424819  2023-01-07 02:14:06               Raffarin   \n",
       "18       El Chadaille  23424819  2023-01-07 02:14:06     %22El+Chadaille%22   \n",
       "19             Lavine  23424819  2023-01-07 02:14:06                 Lavine   \n",
       "\n",
       "    Total_Tweets_Count            lastModified  \n",
       "0              14907.0 2023-01-07 02:14:14.183  \n",
       "1              20900.0 2023-01-07 02:14:14.183  \n",
       "2                  0.0 2023-01-07 02:14:14.183  \n",
       "3                  0.0 2023-01-07 02:14:14.183  \n",
       "4                  0.0 2023-01-07 02:14:14.183  \n",
       "5                  0.0 2023-01-07 02:14:14.183  \n",
       "6                  0.0 2023-01-07 02:14:14.183  \n",
       "7                  0.0 2023-01-07 02:14:14.184  \n",
       "8                  0.0 2023-01-07 02:14:14.184  \n",
       "9              11731.0 2023-01-07 02:14:14.184  \n",
       "10                 0.0 2023-01-07 02:14:14.184  \n",
       "11                 0.0 2023-01-07 02:14:14.184  \n",
       "12                 0.0 2023-01-07 02:14:14.184  \n",
       "13            107711.0 2023-01-07 02:14:14.184  \n",
       "14             50424.0 2023-01-07 02:14:14.184  \n",
       "15             14664.0 2023-01-07 02:14:14.184  \n",
       "16             17321.0 2023-01-07 02:14:14.184  \n",
       "17                 0.0 2023-01-07 02:14:14.184  \n",
       "18                 0.0 2023-01-07 02:14:14.184  \n",
       "19                 0.0 2023-01-07 02:14:14.184  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends[0]['trends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snscrape.modules.twitter import TwitterTrendsScraper\n",
    "import snscrape.modules.twitter as snstwitter\n",
    "\n",
    "def pull_popular_tweets(topics_list,woeid,maximum_tweets):\n",
    "    \n",
    "    for topic in topics_list:\n",
    "        for tweet_cnt, tweets in enumerate(snstwitter.TwitterSearchScraper(topic).get_items()):\n",
    "            temp_dict = {\n",
    "                'tweet_id': tweets.id,\n",
    "                'woeid': woeid,\n",
    "                'tweet_created_at': tweets.date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                'tweet_text': tweets.rawContent.replace(\"'\", \"''\"),\n",
    "                'tweet_lang': tweets.lang,\n",
    "                'tweet_source': tweets.sourceLabel,\n",
    "                'tweets_reply_count': tweets.replyCount,\n",
    "                'tweets_retweet_count': tweets.retweetCount,\n",
    "                'tweets_like_count': tweets.likeCount,\n",
    "                'tweets_search_query': trend.name,\n",
    "                'tweets_location': tweets.coordinates,\n",
    "                'tweets_scrapped_datetime': datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_popular_tweets(topic_list, max_tweets):             \n",
    "    \n",
    "    columns = [ 'pulled_at', 'created_at', 'username', 'user_location', 'region', 'search_type', \n",
    "               'trending_topic', 'retweetcount', 'favorites', 'text', 'hashtags', 'emojis']         # set up columns for dataframes   \n",
    "    \n",
    "    tweets_data_grab = pd.DataFrame(columns = columns)                                  # create empty dataframe    \n",
    "        \n",
    "    for topic in topic_list:                # loop though each trending topic\n",
    "                                                            \n",
    "                                                                                # grab tweets with Cursor\n",
    "        tweets = tweepy.Cursor(api.search, q = topic,                           # search for each trending topic                                 \n",
    "                         lang=\"en\", result_type = 'popular',                    # tweets in english , type is \"recent\"/\"popular\"\n",
    "                          tweet_mode = 'extended').items(max_tweets)            # longer tweets,  grab max_tweets number of tweets\n",
    "        \n",
    "        tweet_list = [tweet for tweet in tweets]                                # create list of tweets\n",
    "                    \n",
    "        tweets_topic = pd.DataFrame(columns = columns)         # create dataframe to put in current top tweets for this town and trending topic\n",
    "            \n",
    "        for tweet in tweet_list:                                      # loop through each tweet that was grabbed\n",
    "            \n",
    "            username = tweet.user.screen_name                                    # store username\n",
    "            user_location = tweet.user.location                                  # store location of user\n",
    "            retweetcount = tweet.retweet_count                                   # store retweet count\n",
    "            favorites = tweet.favorite_count                                     # store favorite count\n",
    "            hashtags = [h['text'].lower() for h in tweet.entities['hashtags']]   # store hashtags    \n",
    "            search_type = 'popular'                                              # store search type\n",
    "            region = \"USA\"                                                       # trending tweets in USA\n",
    "            created_at = tweet.created_at                                        # time tweet created\n",
    "            pulled_at = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")    # time tweet was pulled\n",
    "        \n",
    "            try:                              \n",
    "                text = tweet.retweeted_status.full_text    # store text if it's a retweet\n",
    "            \n",
    "            except AttributeError: \n",
    "                text = tweet.full_text                     # store text if it's a regular tweet\n",
    "                \n",
    "            emoji = list(emojis.get(text))                 # get the emojis\n",
    "            \n",
    "            curr_tweet = [pulled_at, created_at, username, user_location, region,     # store current tweet's data in a list soon to be a row\n",
    "                          search_type, topic, retweetcount, favorites, text, hashtags, emoji]                             \n",
    "        \n",
    "            tweets_topic.loc[len(tweets_topic)] = curr_tweet                         # add current tweet data to dataframe for town and topic         \n",
    "                                \n",
    "        tweets_topic.sort_values(by=['retweetcount', 'favorites'], inplace = True, ascending = False)     # sort the retweet values highest first\n",
    "                                \n",
    "        tweets_data_grab = pd.concat([tweets_data_grab, tweets_topic], ignore_index = True, sort = False)       # concatenate top n to final dataframe\n",
    "        \n",
    "    return tweets_data_grab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snscrape.modules.twitter import TwitterTrendsScraper\n",
    "import snscrape.modules.twitter as snstwitter\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os \n",
    "import mysql.connector\n",
    "import config as cf\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  port=3306,\n",
    "  user=\"sysdba\",\n",
    "  password=\"!mK!ngP@t3\",\n",
    "  database = \"StageTweets_Info\"\n",
    "  )\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "for i, trend in enumerate(TwitterTrendsScraper().get_items()):\n",
    "    print(trend.json())\n",
    "    print(trend.name)\n",
    "    print(trend.domainContext)\n",
    "    for tweet_cnt, tweets in enumerate(snstwitter.TwitterSearchScraper(trend.name).get_items()):\n",
    "        temp_dict={\n",
    "            'tweet_id': tweets.id,\n",
    "            'woeid' : 0000000,\n",
    "            'tweet_created_at': tweets.date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'tweet_text': tweets.rawContent.replace(\"'\",\"''\"),\n",
    "            'tweet_lang': tweets.lang,\n",
    "            'tweet_source': tweets.sourceLabel,\n",
    "            'tweets_reply_count': tweets.replyCount,\n",
    "            'tweets_retweet_count': tweets.retweetCount,\n",
    "            'tweets_like_count': tweets.likeCount,\n",
    "            'tweets_search_query': trend.name,\n",
    "            'tweets_location': tweets.coordinates,\n",
    "            'tweets_scrapped_datetime': datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        values = ', '.join(\"'\" + str(x).replace('/', '_') +\n",
    "                           \"'\" for x in temp_dict.values())\n",
    "\n",
    "        sql = \"INSERT INTO scraped_tweets_stage VALUES ({})\".format(values)\n",
    "        mycursor.execute(sql)\n",
    "        \n",
    "        if tweet_cnt == 1000:\n",
    "            break\n",
    "    if i==20:\n",
    "        break\n",
    "mydb.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ', '.join(\"'\" + str(x).replace('/', '_') +\n",
    "                    \"'\" for x in temp_dict.keys())\n",
    "columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file=open(\"woeid.json\",\"r\")\n",
    "\n",
    "json_file=json.loads(file.read())\n",
    "required_content=[]\n",
    "for content in json_file:\n",
    "    if content['placeType']['name']=='Country':\n",
    "        required_content.append(content)\n",
    "\n",
    "len(required_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"Locations.json\",\"w\") as outfile:\n",
    "    json.dump(required_content,outfile,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file=open(\"Locations.json\",\"r\")\n",
    "\n",
    "json_file=json.loads(file.read())\n",
    "len(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo import UpdateOne\n",
    "import config\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_BaseObject__codec_options': CodecOptions(document_class=dict, tz_aware=False, uuid_representation=UuidRepresentation.UNSPECIFIED, unicode_decode_error_handler='strict', tzinfo=None, type_registry=TypeRegistry(type_codecs=[], fallback_encoder=None), datetime_conversion=DatetimeConversion.DATETIME),\n",
       " '_BaseObject__read_preference': Primary(),\n",
       " '_BaseObject__write_concern': WriteConcern(),\n",
       " '_BaseObject__read_concern': ReadConcern(),\n",
       " '_Database__name': 'stage_tweets',\n",
       " '_Database__client': MongoClient(host=['127.0.0.1:27017'], document_class=dict, tz_aware=False, connect=True),\n",
       " '_timeout': None}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
